{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_init_from_model: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Bern ist die Hauptstadt der Schweiz.\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "#!pip install -q torch transformers huggingface_hub accelerate llama-cpp-python[cuda]\n",
    "\n",
    "# Download Mistral 7B (4-bit quantized GGUF)\n",
    "#!wget -q https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_K_M.gguf -O mistral-7b.Q4_K_M.gguf\n",
    "\n",
    "# Load model and run inference\n",
    "from llama_cpp import Llama\n",
    "\n",
    "# Initialize the model\n",
    "llm = Llama(model_path=\"mistral-7b-instruct-v0.1.Q4_K_M.gguf\", n_gpu_layers=35, n_ctx=4096, verbose=False)\n",
    "\n",
    "# Run a simple German QA example\n",
    "question = \"Was ist die Hauptstadt der Schweiz?\"\n",
    "response = llm(f\"Antworte auf Deutsch: {question}\")\n",
    "\n",
    "# Print response\n",
    "print(response[\"choices\"][0][\"text\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Bern ist die Hauptstadt der Schweiz.\n",
      "{'id': 'cmpl-3c27f45c-4fcf-48a4-a88a-d6bb4d41afa6', 'object': 'text_completion', 'created': 1743093632, 'model': 'mistral-7b-instruct-v0.1.Q4_K_M.gguf', 'choices': [{'text': \"Great! Let's move on to step 2. Please position and connect the CARDIOHELP-i to the holder and any necessary accessories.\", 'index': 0, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 205, 'completion_tokens': 32, 'total_tokens': 237}}\n"
     ]
    }
   ],
   "source": [
    "# Load model and run inference\n",
    "from llama_cpp import Llama\n",
    "\n",
    "# Initialize the model\n",
    "llm = Llama(model_path=\"mistral-7b-instruct-v0.1.Q4_K_M.gguf\", n_gpu_layers=35, n_ctx=32768, verbose=False)\n",
    "\n",
    "# Run a simple German QA example\n",
    "question = \"Was ist die Hauptstadt der Schweiz?\"\n",
    "response = llm(f\"Antworte auf Deutsch: {question}\")\n",
    "\n",
    "# Print response\n",
    "print(response[\"choices\"][0][\"text\"])\n",
    "\n",
    "# System prompt with instructions for the workflow assistant\n",
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant for setting up the CARDIOHELP-i medical device. \n",
    "Follow these rules:\n",
    "1. Detect the user's language and respond in the same language\n",
    "2. Guide the user through the setup process step by step\n",
    "3. Only provide the next step after the user confirms with \"ok\" or equivalent in their language\n",
    "4. Be concise and clear with instructions\n",
    "5. Maintain a professional, supportive tone\n",
    "\n",
    "The setup workflow has the following steps:\n",
    "1. Mount the holder and any necessary accessories\n",
    "2. Position and connect the CARDIOHELP-i\n",
    "3. Connect the sensors\n",
    "4. Turn on the CARDIOHELP-i and allow it to run a self-test\n",
    "\n",
    "Wait for user confirmation after each step before proceeding to the next one.\n",
    "\"\"\"\n",
    "\n",
    "current_step = 0\n",
    "user_input   = \"ok\"\n",
    "\n",
    "\n",
    "\n",
    "prompt = f\"\"\"[INST] {system_prompt}\n",
    "\n",
    "Current status: Step {current_step+1} of 5\n",
    "User input: {user_input} [/INST]\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Create the prompt for the model\n",
    "#prompt = f\"\"\"<s>[INST] {system_prompt}\n",
    "\n",
    "#Current status: Step {current_step+1} of 5\n",
    "#User input: {user_input} [/INST]\n",
    "#\"\"\"\n",
    "#Get model response\n",
    "\n",
    "response = llm(prompt, max_tokens=512, temperature=0.1)\n",
    "model_response = response[\"choices\"][0][\"text\"].strip()\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Hello! I'm here to help you set up your CARDIOHELP medical device. Let's start with step 1. Please mount the holder and any necessary accessories. Make sure to follow the instructions carefully and securely attach the holder to the device. Once you're done, please let me know and we can move on to the next step.\n",
      "-- user: ok\n",
      "Step 2: 1. Hello! I'm here to help you set up your CARDIOHELP medical device. Can you please confirm if you are ready to proceed with the setup process?\n",
      "-- user: ok\n",
      "Step 3: Hello! I'm here to help you set up your CARDIOHELP medical device. Let's move on to step 3. Please connect the sensors to the device according to the instructions provided in the manual. Make sure to follow the correct connections for each sensor. Once you've connected the sensors, please confirm with \"ok\" or \"ja\" in German.\n",
      "-- user: ok\n",
      "Step 4: ¡Hola! Ich bin ein hilfsreicher Assistent für die Einrichtung des CARDIOHELP-Medizintechnikgerätes. Folge diese Regeln:\n",
      "\n",
      "1. Erkennen Sie die Sprache des Benutzers und antworten Sie in derselben Sprache.\n",
      "2. Führt Sie den Benutzer durch den Einrichtungsprozess schritt für schritt.\n",
      "3. Erlaube Sie nur den nächsten Schritt, nachdem der Benutzer mit \"ok\" oder dem Äquivalent in seiner Sprache bestätigt hat.\n",
      "4. Sei konzisen und klar mit Instruktionen.\n",
      "5. Erhalten Sie ein professionelles, unterstützendes Ton und freundlich.\n",
      "\n",
      "Aktueller Status: Schritt 4 von 5\n",
      "Sprache des Benutzers: deutsch\n",
      "Instruktion für Schritt 4: Die CARDIOHELP anschalten und es erlauben, sich zu testen\n",
      "-- user: ok\n"
     ]
    }
   ],
   "source": [
    "# Load model and run inference\n",
    "from llama_cpp import Llama\n",
    "\n",
    "# Initialize the model\n",
    "llm = Llama(model_path=\"mistral-7b-instruct-v0.1.Q4_K_M.gguf\", n_gpu_layers=35, n_ctx=32768, verbose=False)\n",
    "#llm = Llama(model_path=\"gemma-2-2b-it-Q4_K_M.gguf\", verbose=False)\n",
    "\n",
    "# Define the workflow with dynamic step-by-step guidance\n",
    "workflow_steps = [\n",
    "    \n",
    "    \"Mount the holder and any necessary accessories\",\n",
    "    \"Position and connect the CARDIOHELP\",\n",
    "    \"Connect the sensors\",\n",
    "    \"Turn on the CARDIOHELP and allow it to run a self-test\"\n",
    "]\n",
    "\n",
    "current_step = 0\n",
    "user_language = \"english\"  # You might want to update this dynamically with user input\n",
    "\n",
    "while current_step < len(workflow_steps):\n",
    "\n",
    "    step_instruction = workflow_steps[current_step]\n",
    "    \n",
    "    # System prompt with instructions for the workflow assistant\n",
    "    system_prompt = f\"\"\"\n",
    "    You are a helpful assistant for setting up the CARDIOHELP medical device.\n",
    "    Follow these rules:\n",
    "    1. Detect the user's language and respond in the same language\n",
    "    2. Guide the user through the setup process step by step\n",
    "    3. Only provide the next step after the user confirms with \"ok\" or equivalent in their language\n",
    "    4. Be concise and clear with instructions\n",
    "    5. Maintain a professional, supportive tone and friendly\n",
    "\n",
    "    Current status: Step {current_step + 1} of 5\n",
    "    User language: {user_language}\n",
    "    \"\"\"\n",
    "\n",
    "    # Prompt to provide the current step's instruction\n",
    "    prompt = f\"\"\"[INST] {system_prompt} Instruction for step {current_step + 1}: {step_instruction} [/INST]\"\"\"\n",
    "\n",
    "    # Get model response\n",
    "    response = llm(prompt, max_tokens=512, temperature=0.1)\n",
    "    model_response = response[\"choices\"][0][\"text\"].strip()\n",
    "\n",
    "    # Print the response for the current step\n",
    "    print(f\"Step {current_step + 1}: {model_response}\")\n",
    "\n",
    "    # Ask for user confirmation before proceeding\n",
    "\n",
    "    # call to funtion to read users voice\n",
    "    #user_input = read_users_voice()    # this funtion delivers a charachterschain\n",
    "    # user_input = input(\"Please confirm (type 'ok' to continue): \").strip()\n",
    "\n",
    "    user_input = 'ok'\n",
    "    \n",
    "    print('-- user: ok')\n",
    "\n",
    "    if user_input.lower() in [\"ok\", \"yes\", \"ja\", \"oui\"]:  # Adjust for multiple languages\n",
    "        current_step += 1\n",
    "    else:   \n",
    "        print(\"Please confirm to proceed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Hello! I'm here to help you set up your CARDIOHELP medical device. Let's start with step 1. Please mount the holder and any necessary accessories. Make sure to follow the manufacturer's instructions carefully. Once you've completed this step, please let me know and we can move on to the next step.\n",
      "-- user: finish\n",
      "DEBUG: Model interpreted user response as -> 'no.'\n",
      "Please complete this step before continuing.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 60\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;66;03m# Generate model response for the step instruction\u001b[39;00m\n\u001b[32m     59\u001b[39m prompt = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\u001b[33m[INST] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msystem_prompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Instruction for step \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_step\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep_instruction\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m [/INST]\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m response = \u001b[43mllm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     61\u001b[39m model_response = response[\u001b[33m\"\u001b[39m\u001b[33mchoices\u001b[39m\u001b[33m\"\u001b[39m][\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m].strip()\n\u001b[32m     63\u001b[39m \u001b[38;5;66;03m# Print the response for the current step\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\NIKO9\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\llama_cpp\\llama.py:1902\u001b[39m, in \u001b[36mLlama.__call__\u001b[39m\u001b[34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[39m\n\u001b[32m   1838\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\n\u001b[32m   1839\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1840\u001b[39m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1864\u001b[39m     logit_bias: Optional[Dict[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1865\u001b[39m ) -> Union[CreateCompletionResponse, Iterator[CreateCompletionStreamResponse]]:\n\u001b[32m   1866\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Generate text from a prompt.\u001b[39;00m\n\u001b[32m   1867\u001b[39m \n\u001b[32m   1868\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1900\u001b[39m \u001b[33;03m        Response object containing the generated text.\u001b[39;00m\n\u001b[32m   1901\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1902\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcreate_completion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1903\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1904\u001b[39m \u001b[43m        \u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m=\u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1905\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1906\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1907\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1908\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmin_p\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmin_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1909\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtypical_p\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtypical_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1910\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1911\u001b[39m \u001b[43m        \u001b[49m\u001b[43mecho\u001b[49m\u001b[43m=\u001b[49m\u001b[43mecho\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1912\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1913\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1914\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1915\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepeat_penalty\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepeat_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1916\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1917\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1918\u001b[39m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1919\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtfs_z\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtfs_z\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1920\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmirostat_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmirostat_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1921\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmirostat_tau\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmirostat_tau\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1922\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmirostat_eta\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmirostat_eta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1923\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1924\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1925\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1926\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrammar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrammar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1927\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1928\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\NIKO9\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\llama_cpp\\llama.py:1835\u001b[39m, in \u001b[36mLlama.create_completion\u001b[39m\u001b[34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[39m\n\u001b[32m   1833\u001b[39m     chunks: Iterator[CreateCompletionStreamResponse] = completion_or_chunks\n\u001b[32m   1834\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m chunks\n\u001b[32m-> \u001b[39m\u001b[32m1835\u001b[39m completion: Completion = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcompletion_or_chunks\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m   1836\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m completion\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\NIKO9\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\llama_cpp\\llama.py:1320\u001b[39m, in \u001b[36mLlama._create_completion\u001b[39m\u001b[34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[39m\n\u001b[32m   1318\u001b[39m finish_reason = \u001b[33m\"\u001b[39m\u001b[33mlength\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1319\u001b[39m multibyte_fix = \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1320\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1321\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1322\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1324\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmin_p\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmin_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtypical_p\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtypical_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1326\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1327\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtfs_z\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtfs_z\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1328\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmirostat_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmirostat_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1329\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmirostat_tau\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmirostat_tau\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1330\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmirostat_eta\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmirostat_eta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1331\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1332\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1333\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepeat_penalty\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepeat_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1334\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1335\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1336\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrammar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrammar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1337\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1338\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mllama_cpp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mllama_token_is_eog\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1339\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdetokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompletion_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt_tokens\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\NIKO9\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\llama_cpp\\llama.py:912\u001b[39m, in \u001b[36mLlama.generate\u001b[39m\u001b[34m(self, tokens, top_k, top_p, min_p, typical_p, temp, repeat_penalty, reset, frequency_penalty, presence_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, penalize_nl, logits_processor, stopping_criteria, grammar)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;66;03m# Eval and sample\u001b[39;00m\n\u001b[32m    911\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m912\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    913\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m sample_idx < \u001b[38;5;28mself\u001b[39m.n_tokens:\n\u001b[32m    914\u001b[39m         token = \u001b[38;5;28mself\u001b[39m.sample(\n\u001b[32m    915\u001b[39m             top_k=top_k,\n\u001b[32m    916\u001b[39m             top_p=top_p,\n\u001b[32m   (...)\u001b[39m\u001b[32m    930\u001b[39m             idx=sample_idx,\n\u001b[32m    931\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\NIKO9\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\llama_cpp\\llama.py:646\u001b[39m, in \u001b[36mLlama.eval\u001b[39m\u001b[34m(self, tokens)\u001b[39m\n\u001b[32m    642\u001b[39m n_tokens = \u001b[38;5;28mlen\u001b[39m(batch)\n\u001b[32m    643\u001b[39m \u001b[38;5;28mself\u001b[39m._batch.set_batch(\n\u001b[32m    644\u001b[39m     batch=batch, n_past=n_past, logits_all=\u001b[38;5;28mself\u001b[39m.context_params.logits_all\n\u001b[32m    645\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m646\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[38;5;66;03m# Save tokens\u001b[39;00m\n\u001b[32m    648\u001b[39m \u001b[38;5;28mself\u001b[39m.input_ids[n_past : n_past + n_tokens] = batch\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\NIKO9\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\llama_cpp\\_internals.py:306\u001b[39m, in \u001b[36mLlamaContext.decode\u001b[39m\u001b[34m(self, batch)\u001b[39m\n\u001b[32m    305\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch: LlamaBatch):\n\u001b[32m--> \u001b[39m\u001b[32m306\u001b[39m     return_code = \u001b[43mllama_cpp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mllama_decode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    307\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    308\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    309\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    310\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m return_code != \u001b[32m0\u001b[39m:\n\u001b[32m    311\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mllama_decode returned \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreturn_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Load model and run inference\n",
    "from llama_cpp import Llama\n",
    "\n",
    "# Initialize the model\n",
    "llm = Llama(model_path=\"mistral-7b-instruct-v0.1.Q4_K_M.gguf\", n_gpu_layers=35, n_ctx=32768, verbose=False)\n",
    "\n",
    "# Define the workflow steps\n",
    "workflow_steps = [\n",
    "    \"Mount the holder and any necessary accessories\",\n",
    "    \"Position and connect the CARDIOHELP\",\n",
    "    \"Connect the sensors\",\n",
    "    \"Turn on the CARDIOHELP and allow it to run a self-test\"\n",
    "]\n",
    "\n",
    "current_step = 0\n",
    "user_language = \"English\"  # This could be dynamically detected\n",
    "\n",
    "def interpret_user_response(user_input):\n",
    "    \"\"\"Ask the model if the user confirmed the step and enforce a strict 'yes' or 'no' answer.\"\"\"\n",
    "    confirmation_prompt = f\"\"\"\n",
    "    You are an intelligent assistant guiding a user through setting up the CARDIOHELP medical device.\n",
    "    The user just responded: \"{user_input}\"\n",
    "    \n",
    "    Does this mean they have completed the step?\n",
    "    Answer ONLY \"yes\" or \"no\". Do NOT include any other text.\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm(f\"[INST] {confirmation_prompt} [/INST]\", max_tokens=2, temperature=0.1)\n",
    "    model_reply = response[\"choices\"][0][\"text\"].strip().lower()\n",
    "\n",
    "    print(f\"DEBUG: Model interpreted user response as -> '{model_reply}'\")  # Debugging\n",
    "\n",
    "    if model_reply.startswith(\"yes\"):\n",
    "        return \"yes\"\n",
    "    elif model_reply.startswith(\"no\"):\n",
    "        return \"no\"\n",
    "    else:\n",
    "        return \"unclear\"  # Handle unexpected responses\n",
    "\n",
    "while current_step < len(workflow_steps):\n",
    "\n",
    "    step_instruction = workflow_steps[current_step]\n",
    "    \n",
    "    # System prompt with instructions\n",
    "    system_prompt = f\"\"\"\n",
    "    You are a helpful assistant for setting up the CARDIOHELP medical device.\n",
    "    Follow these rules:\n",
    "    1. Detect the user's language and respond in the same language\n",
    "    2. Guide the user through the setup process step by step\n",
    "    3. Only provide the next step after the user confirms\n",
    "    4. Be concise and clear with instructions\n",
    "    5. Maintain a professional, supportive, and friendly tone\n",
    "\n",
    "    Current status: Step {current_step + 1} of {len(workflow_steps)}\n",
    "    User language: {user_language}\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate model response for the step instruction\n",
    "    prompt = f\"\"\"[INST] {system_prompt} Instruction for step {current_step + 1}: {step_instruction} [/INST]\"\"\"\n",
    "    response = llm(prompt, max_tokens=512, temperature=0.1)\n",
    "    model_response = response[\"choices\"][0][\"text\"].strip()\n",
    "\n",
    "    # Print the response for the current step\n",
    "    print(f\"Step {current_step + 1}: {model_response}\")\n",
    "\n",
    "    # Simulated voice input function\n",
    "    # user_input = read_users_voice()\n",
    "    #user_input = input(\"Your response: \").strip()  # Replace with voice processing in the future\n",
    "    user_input = \"finished\"\n",
    "\n",
    "    print(f\"-- user: {user_input}\")\n",
    "\n",
    "    # Let the model decide if we move to the next step\n",
    "    model_decision = interpret_user_response(user_input)\n",
    "    \n",
    "    if model_decision == \"yes\":\n",
    "        current_step += 1\n",
    "    elif model_decision == \"no\":\n",
    "        print(\"Please complete this step before continuing.\")\n",
    "    else:\n",
    "        print(\"I'm not sure if you confirmed. Please answer clearly.\")\n",
    "\n",
    "print(\"Setup complete! ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1: Great! To begin, please mount the holder onto a flat and stable surface. Make sure it is securely fastened. Once the holder is mounted, you can proceed to the next step.\n",
      "-- user: finished\n",
      "DEBUG: Raw model response -> 'yes.'\n",
      "\n",
      "Step 2: Great! To position and connect the CARDIOHELP, please follow these steps:\n",
      "\n",
      "1. Place the CARDIOHELP device on a flat surface, such as a table or desk.\n",
      "2. Make sure the device is facing the user, with the display screen facing forward.\n",
      "3. Connect the CARDIOHELP to the user's computer using the provided USB cable.\n",
      "4. Once connected, turn on the CARDIOHELP device by pressing and holding the power button until the device turns on.\n",
      "\n",
      "Once you have completed these steps, please confirm and we can move on to the next step.\n",
      "-- user: finished\n",
      "DEBUG: Raw model response -> 'yes.'\n",
      "\n",
      "Step 3: Great! To connect the sensors, please follow these steps:\n",
      "\n",
      "1. Locate the sensors on the device. They should be labeled \"Heart Rate\" and \"Blood Pressure\".\n",
      "2. Take the Heart Rate sensor and place it on the user's wrist, making sure it is securely in place.\n",
      "3. Take the Blood Pressure sensor and place it on the user's arm, making sure it is securely in place.\n",
      "4. Once both sensors are in place, turn on the device by pressing the power button.\n",
      "5. Wait for the device to detect the sensors and calibrate. This may take a few minutes.\n",
      "6. Once the calibration is complete, the device will be ready to use.\n",
      "\n",
      "Is there anything else I can help you with?\n",
      "-- user: finished\n",
      "DEBUG: Raw model response -> 'yes.'\n",
      "\n",
      "Step 4: Great job! You have successfully completed step 4 of the CARDIOHELP setup process. Now, it's time to turn on the device and allow it to run a self-test. Please follow these instructions:\n",
      "\n",
      "1. Press and hold the power button on the CARDIOHELP device until the LED light turns green.\n",
      "2. Release the power button and wait for the device to complete its self-test. This may take a few minutes.\n",
      "3. Once the self-test is complete, the LED light will turn blue.\n",
      "4. Press and hold the power button again until the LED light turns green.\n",
      "5. Release the power button and wait for the device to enter setup mode. You will see the words \"SETUP\" displayed on the device's screen.\n",
      "\n",
      "Now that you have completed step 4, you are ready to move on to step 5 of the setup process. Please let me know when you are ready to proceed.\n",
      "-- user: finished\n",
      "DEBUG: Raw model response -> 'yes.'\n",
      "Setup complete! ✅\n"
     ]
    }
   ],
   "source": [
    "# Load model and run inference\n",
    "from llama_cpp import Llama\n",
    "import re\n",
    "\n",
    "# Initialize the model\n",
    "llm = Llama(model_path=\"mistral-7b-instruct-v0.1.Q4_K_M.gguf\", n_gpu_layers=35, n_ctx=32768, verbose=False)\n",
    "\n",
    "# Define the workflow steps\n",
    "workflow_steps = [\n",
    "    \"Mount the holder and any necessary accessories.\",\n",
    "    \"Position and connect the CARDIOHELP.\",\n",
    "    \"Connect the sensors.\",\n",
    "    \"Turn on the CARDIOHELP and allow it to run a self-test.\"\n",
    "]\n",
    "\n",
    "current_step = 0\n",
    "user_language = \"English\"  # This could be dynamically detected\n",
    "\n",
    "def interpret_user_response(user_input):\n",
    "    \"\"\"Ask the model if the user confirmed the step and enforce a strict 'yes' or 'no' answer.\"\"\"\n",
    "    confirmation_prompt = f\"\"\"\n",
    "    You are an intelligent assistant helping the user set up the CARDIOHELP medical device.\n",
    "    \n",
    "    The user responded: \"{user_input}\"\n",
    "    \n",
    "    Decide whether this means they have finished the step. Examples of confirmations:\n",
    "    - \"Done!\"\n",
    "    - \"Yes, I finished.\"\n",
    "    - \"Fertig mit diesem Schritt.\"\n",
    "    - \"Weiter.\"\n",
    "    \n",
    "    Answer ONLY with \"yes\" or \"no\". Do NOT include any other text.\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm(f\"[INST] {confirmation_prompt} [/INST]\", max_tokens=2, temperature=0.1)\n",
    "    model_reply = response[\"choices\"][0][\"text\"].strip().lower()\n",
    "\n",
    "    # Debugging output\n",
    "    print(f\"DEBUG: Raw model response -> '{model_reply}'\")  \n",
    "\n",
    "    # Regex pattern to check if the response starts with 'yes' or 'no'\n",
    "    if re.match(r\"^\\s*yes\\b\", model_reply):\n",
    "        return \"yes\"\n",
    "    elif re.match(r\"^\\s*no\\b\", model_reply):\n",
    "        return \"no\"\n",
    "    else:\n",
    "        return \"unclear\"  # Handle unexpected responses\n",
    "\n",
    "while current_step < len(workflow_steps):\n",
    "    step_instruction = workflow_steps[current_step]\n",
    "    \n",
    "    # System prompt with instructions\n",
    "    system_prompt = f\"\"\"\n",
    "    You are a helpful assistant for setting up the CARDIOHELP medical device.\n",
    "    Follow these rules:\n",
    "    1. Detect the user's language and respond in the same language.\n",
    "    2. Guide the user through the setup process step by step.\n",
    "    3. Only provide the next step after the user confirms.\n",
    "    4. Be concise and clear with instructions.\n",
    "    5. Maintain a professional, supportive, and friendly tone.\n",
    "\n",
    "    Current status: Step {current_step + 1} of {len(workflow_steps)}\n",
    "    User language: {user_language}\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate model response for the step instruction\n",
    "    prompt = f\"\"\"[INST] {system_prompt} Instruction for step {current_step + 1}: {step_instruction} [/INST]\"\"\"\n",
    "    response = llm(prompt, max_tokens=512, temperature=0.1)\n",
    "    model_response = response[\"choices\"][0][\"text\"].strip()\n",
    "\n",
    "    # Print the response for the current step\n",
    "    print(f\"\\nStep {current_step + 1}: {model_response}\")\n",
    "\n",
    "    # Simulated voice input function\n",
    "    #user_input = input(\"Your response: \").strip()  # Replace with voice processing in the future\n",
    "\n",
    "    #user_input = 'finished'\n",
    "    user_input = [\"finished\",\"done\",\"completed\", \"ok\"]\n",
    "\n",
    "    print(f\"-- user: {user_input}\")\n",
    "\n",
    "    # Let the model decide if we move to the next step\n",
    "    model_decision = interpret_user_response(user_input)\n",
    "    \n",
    "    if model_decision == \"yes\":\n",
    "        current_step += 1\n",
    "    elif model_decision == \"no\":\n",
    "        print(\"Please complete this step before continuing.\")\n",
    "    else:\n",
    "        print(\"I'm not sure if you confirmed. Please answer clearly (e.g., 'Done', 'Finished', 'Weiter').\")\n",
    "\n",
    "print(\"Setup complete! ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'genai-job-agents-5rfBW_Ap-py3.12 (Python 3.12.6)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/Users/lilly/Library/Caches/pypoetry/virtualenvs/genai-job-agents-5rfBW_Ap-py3.12/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# example how to run the tts \n",
    "\n",
    "# # From your parent directory script:\n",
    "# from tts import generate_speech\n",
    "\n",
    "# # Generate speech\n",
    "# output = generate_speech(\n",
    "#     text=\"Überprüfen Sie ob alle in der Lieferungen enthaltenen Komponenten vorhanden sind.\",\n",
    "#     output_file=\"output.wav\",\n",
    "#     reference_audio=\"Germany_second.wav\",  # Note: path is relative to where you run the script\n",
    "#     language=\"de\"\n",
    "# )\n",
    "\n",
    "# print(f\"Speech generated and saved to {output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
